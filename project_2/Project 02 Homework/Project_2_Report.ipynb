{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.datasets import load_diabetes\n",
    "from scipy.stats import uniform, randint\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import timeit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "\n",
    "from tpot import TPOTRegressor, TPOTClassifier\n",
    "import tpot\n",
    "\n",
    "np.random.seed(8675309)  # seed courtesy of Tommy Tutone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you're going to apply hyperparameter optimization to both a regression and a classification problem. It looks like a lot to do below, but it's mostly a matter of modifying code from the presentation. \n",
    "\n",
    "## Objective\n",
    "\n",
    "For each of the models in problems 1 and 2 below, apply the following 4 tuning methods from the presentation: GridSearchCV, RandomSearchCV, BayesianOptimization, and TPOT.\n",
    "* **For TPOT**: In Problem 1 do only hyperparameter optimization. In Problem 2 do **both** hyperparameter optimization and also run TPOT and let it choose the model. See the presentation for examples of both.\n",
    "\n",
    "### What to submit\n",
    "\n",
    "For each problem you need to include the following:\n",
    "\n",
    "1. A pandas table that reports:\n",
    "    * The best parameters for each tuning method\n",
    "    * The optimized score from the test data\n",
    "    * The number of model fits used in the optimization\n",
    "2. A brief discussion about which hyperparameter optimization approach worked best\n",
    "\n",
    "### Notes:\n",
    "* **For problem 1**: your pandas table should include the best parameters for each of the 4 tuning methods above.\n",
    "* **For problem 2**: your pandas table should include the best parameters for each of the 5 tuning methods (the 4 methods above and the TPOT model search).\n",
    "* **For GridSearchCV**: you should include at least 2 or 3 values for each hyperparameter and one of those values should be the default.\n",
    "* **For BayesianOptimization**: you'll have to use `int()` or `bool()` to cast the float values of the hyperparameters inside your `cross_cv()` function.\n",
    "* **For TPOT**: you should use a finer grid than for GridSearchCV, but not more than 10 to 20 possible values for each hyperparameter.  You could lower the number of possible values to keep the search space smaller.\n",
    "    * If your code is too slow you can reduce the number of cross-validation folds to 3 and if your dataset is really large you can randomly choose a smaller subset of the rows.\n",
    "* Use section headers to label your work.  Your summary / discussion should be more than simply \"XYZ is the best model\", but it also shouldn't be more than a few paragraphs and a table.\n",
    "\n",
    "\n",
    "### Regarding data\n",
    "\n",
    "* You can use either the specified dataset or you can choose your own.  \n",
    "    * If you use your own data it should have at least 500 rows and 10 features.  \n",
    "    * If your data has categorical features you'll need \"one hot\" encode it (convert categorical features into multiple binary features).  <a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\">Here is a nice tutorial</a>.  For categories with only two values you can remove one of the two hot encoded columns.\n",
    "* If you do want to use your own data, we suggest first getting things working with the suggested datasets.  Finding, cleaning, and preparing data can take a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Optimize Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimized hyperparameters for a random forest regression model. \n",
    "\n",
    "You may use either the diabetes data used in the presentation or a dataset that you choose.  **You do not need to include the TPOT general search for this problem** (use TPOT to optimize RandomForestRegressor, but don't run TOPT to choose a model). Here are ranges for a subset of the hyperparameters:\n",
    "\n",
    "Hyperparameter |Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 10 to 150\n",
    "max_features | continuous / float | 1.0 | 0.05 to 1.0\n",
    "min_samples_split | discrete / integer | 2 | 2 to 20\n",
    "min_samples_leaf | discrete / integer | 1 | 1 to 20\n",
    "bootstrap | discrete / boolean | True | True, False\n",
    "\n",
    "\n",
    "You can add other hyperparameters to the optimization if you wish.\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">Documentation for sklearn RandomForestRegressor</a>\n",
    "\n",
    "<font color = \"blue\"> *** 15 points: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and define x, y\n",
    "diabetes = load_diabetes()\n",
    "xd = np.array(diabetes.data)\n",
    "yd = np.array(diabetes.target)\n",
    "\n",
    "# split data into train/test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(xd, yd, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(model, best_params, x_test = x_test, y_test = y_test):\n",
    "    \"\"\"\n",
    "    Function to calculate scores for both regression models and classifiers. It works with only randomforest regressor, but could be changed\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_test)  \n",
    "    results = best_params\n",
    "    condition = False\n",
    "\n",
    "    try:\n",
    "        condition = isinstance(model.estimator, sklearn.ensemble.forest.RandomForestRegressor)\n",
    "        \n",
    "    except:\n",
    "        condition = isinstance(model, (sklearn.ensemble.forest.RandomForestRegressor, tpot.tpot.TPOTRegressor))\n",
    "    \n",
    "    if condition == True:\n",
    "        r_squared = model.score(x_test,y_test)\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        results['rmse'] = rmse\n",
    "        results['r_squared'] = r_squared\n",
    "    \n",
    "    else:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        sensitivity = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "        \n",
    "        results['accuracy'] = accuracy\n",
    "        results['precision'] = precision\n",
    "        results['sensitivity'] = sensitivity\n",
    "\n",
    "    return results\n",
    "\n",
    "def cv_score_rf(hyp_parameters):\n",
    "    \"\"\"\n",
    "    Perform CV on RF\n",
    "    \"\"\"\n",
    "    hyp_parameters = hyp_parameters[0]\n",
    "    rf_model = RandomForestRegressor(n_estimators=int(hyp_parameters[0]),\n",
    "                                 max_features=hyp_parameters[1],\n",
    "                                 min_samples_split=int(hyp_parameters[2]),\n",
    "                                 min_samples_leaf=int(hyp_parameters[3]),\n",
    "                                 bootstrap=bool(hyp_parameters[4]))\n",
    "    scores = cross_val_score(rf_model,\n",
    "                             X=x_train,\n",
    "                             y=y_train,\n",
    "                             cv=KFold(n_splits=5))\n",
    "    return np.array(scores.mean())\n",
    "\n",
    "def cv_score_xgb(hyp_parameters):\n",
    "    \"\"\"\n",
    "    Perform CV on XGB\n",
    "    \"\"\"\n",
    "    hyp_parameters = hyp_parameters[0]\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                 learning_rate=hyp_parameters[0],\n",
    "                                 max_depth=int(hyp_parameters[1]),\n",
    "                                 n_estimators=int(hyp_parameters[2]),\n",
    "                                 subsample=hyp_parameters[3],\n",
    "                                 min_child_weight=int(hyp_parameters[4]),\n",
    "                                 reg_alpha=hyp_parameters[5],\n",
    "                                 reg_lambda=hyp_parameters[6],\n",
    "                                njobs=-1)\n",
    "    scores = cross_val_score(xgb_model,\n",
    "                             X=x_train,\n",
    "                             y=y_train,\n",
    "                             cv=KFold(n_splits=5))\n",
    "    return np.array(scores.mean())  # return average of 5-fold scores\n",
    "\n",
    "def lines_that_start_with(string, fp):\n",
    "    return [line for line in fp if line.startswith(string)]\n",
    "\n",
    "def lines_that_contain(string, fp):\n",
    "    return [line for line in fp if string in line]\n",
    "\n",
    "def optimize_model(model, \n",
    "                   opt_type, \n",
    "                   x_train = x_train, \n",
    "                   y_train = y_train, \n",
    "                   x_test = x_test, \n",
    "                   y_test = y_test, \n",
    "                   use_parallel = True,\n",
    "                   cv = 5,\n",
    "                   n = 3):\n",
    "\n",
    "    # Set params for RF    \n",
    "    if isinstance(model, RandomForestRegressor):\n",
    "        n_estimators = np.linspace(10,150, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(10,151)\n",
    "        max_features =  np.linspace(0.05,1, n).tolist() if opt_type != 'randomcv' else uniform(0.05,1)\n",
    "        min_samples_split = np.linspace(2,20, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(2,21)\n",
    "        min_samples_leaf = np.linspace(1,20, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(1,21)\n",
    "        bootstrap = np.array([True, False], dtype=bool).tolist() if opt_type != 'randomcv' else [True,False]\n",
    "    \n",
    "        params = {\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_features\": max_features,\n",
    "            \"min_samples_split\": min_samples_split,\n",
    "            \"min_samples_leaf\": min_samples_leaf,\n",
    "            \"bootstrap\": bootstrap\n",
    "        }\n",
    "\n",
    "    # Set RFS for XGB\n",
    "    else: \n",
    "        n_estimators = np.linspace(50,150, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(50,151)\n",
    "        max_depth =  np.linspace(1,10, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(1,11)\n",
    "        min_child_weight = np.linspace(1,20, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(1,21)\n",
    "        learning_rate = np.linspace(0.001,1, n).tolist() if opt_type != 'randomcv' else uniform(0.001,1)\n",
    "        subsample = np.linspace(0.05,1, n).tolist() if opt_type != 'randomcv' else uniform(0.05,0.9)\n",
    "        reg_lambda = np.linspace(0,5, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(0,5)\n",
    "        reg_alpha = np.arange(1,6, n, dtype=np.int).tolist() if opt_type != 'randomcv' else randint(0,5)\n",
    "    \n",
    "        params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"subsample\": subsample,\n",
    "            \"min_child_weight\": min_child_weight,\n",
    "            \"reg_lambda\": reg_lambda,\n",
    "            \"reg_alpha:\": reg_alpha\n",
    "        }\n",
    "    \n",
    "    # Specify to use parallel processing\n",
    "    n_jobs = -1 if use_parallel == True else 1\n",
    "    \n",
    "    # if tuning method == grid search\n",
    "    if opt_type == 'gridcv':\n",
    "        print(\"Optimizing hyperparameters with GridSearchCV...\")\n",
    "        grid_search = GridSearchCV(model,\n",
    "                           param_grid=params,\n",
    "                           cv=cv,\n",
    "                           verbose=1,\n",
    "                           n_jobs=n_jobs,\n",
    "                           return_train_score=True)\n",
    "        \n",
    "        grid_search.fit(x_train, y_train)\n",
    "        best_params = grid_search.best_params_\n",
    "        results = calculate_scores(grid_search, best_params)\n",
    "        return(results)\n",
    "\n",
    "    # if tuning method == random CV\n",
    "    elif opt_type == 'randomcv':\n",
    "        print(\"Optimizing hyperparameters with RandomSearchCV...\")\n",
    "        random_search = RandomizedSearchCV(\n",
    "            model,\n",
    "            param_distributions=params,\n",
    "            random_state=8675309,\n",
    "            n_iter=10,\n",
    "            cv=cv,\n",
    "            verbose=1,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True)\n",
    "        \n",
    "        random_search.fit(x_train, y_train)\n",
    "        best_params = random_search.best_params_\n",
    "        results = calculate_scores(random_search, best_params)\n",
    "        return(results)\n",
    "    \n",
    "    # if tuning method == Bayesian optimization\n",
    "    elif opt_type == \"bayes\":\n",
    "        print(\"Optimizing hyperparameters with Bayesian Optimization...\")\n",
    "        \n",
    "        # if model is RF\n",
    "        if isinstance(model, RandomForestRegressor):\n",
    "            hp_bounds = [{'name': 'n_estimators', 'type': 'discrete', 'domain': (min(n_estimators), max(n_estimators))}, \n",
    "            {'name': 'max_features','type': 'continuous','domain': (min(max_features), max(max_features))}, \n",
    "            {'name': 'min_samples_split','type': 'discrete','domain': (min(min_samples_split), max(min_samples_split))}, \n",
    "            {'name': 'min_samples_leaf','type': 'discrete','domain': (min(min_samples_leaf), max(min_samples_leaf))}, \n",
    "            {'name': 'bootstrap','type': 'discrete','domain': (True, False)}]\n",
    "            cv_score = cv_score_rf\n",
    "        \n",
    "        # if model is XGB\n",
    "        else:\n",
    "            hp_bounds = [{'name': 'learning_rate','type': 'continuous','domain': (min(learning_rate), max(learning_rate))}, \n",
    "            {'name': 'max_depth','type': 'discrete','domain': (min(max_depth), max(max_depth))}, \n",
    "            {'name': 'n_estimators','type': 'discrete','domain': (min(n_estimators), max(n_estimators))}, \n",
    "            {'name': 'subsample','type': 'continuous','domain': (min(subsample), max(subsample))}, \n",
    "            {'name': 'min_child_weight','type': 'discrete','domain': (min(min_child_weight), max(min_child_weight))}, \n",
    "            {'name': 'reg_alpha','type': 'continuous','domain': (min(reg_alpha), max(reg_alpha))}, \n",
    "            {'name': 'reg_lambda','type': 'continuous','domain': (min(reg_lambda), max(reg_lambda))}]\n",
    "            cv_score = cv_score_xgb\n",
    "\n",
    "        # create optmizer\n",
    "        optimizer = BayesianOptimization(f=cv_score,\n",
    "                                         domain=hp_bounds,\n",
    "                                         model_type='GP',\n",
    "                                         acquisition_type='EI',\n",
    "                                         acquisition_jitter=0.05,\n",
    "                                         exact_feval=True,\n",
    "                                         maximize=True,\n",
    "                                         verbosity=True,\n",
    "                                        njobs=n_jobs)\n",
    "\n",
    "        optimizer.run_optimization(max_iter=20,verbosity=False)\n",
    "        best_params = {}\n",
    "\n",
    "        # if model is RF, convert continuous/discrete vals\n",
    "        if isinstance(model, RandomForestRegressor):\n",
    "            for i in range(len(hp_bounds)):\n",
    "                if hp_bounds[i]['type'] == 'continuous':\n",
    "                    best_params[hp_bounds[i]['name']] = optimizer.x_opt[i]\n",
    "                elif hp_bounds[i]['type'] == 'discrete' and hp_bounds[i]['name'] != 'bootstrap':\n",
    "                    best_params[hp_bounds[i]['name']] = int(optimizer.x_opt[i])\n",
    "                else:\n",
    "                    best_params[hp_bounds[i]['name']] = bool(optimizer.x_opt[i])\n",
    "    \n",
    "            bayopt_search = RandomForestRegressor(**best_params)\n",
    "    \n",
    "        # if model is XGB, do same conversion\n",
    "        else:\n",
    "            for i in range(len(hp_bounds)):\n",
    "                if hp_bounds[i]['type'] == 'continuous':\n",
    "                    best_params[hp_bounds[i]['name']] = optimizer.x_opt[i]\n",
    "                else:\n",
    "                    best_params[hp_bounds[i]['name']] = int(optimizer.x_opt[i])\n",
    "\n",
    "            bayopt_search =  xgb.XGBClassifier(objective=\"binary:logistic\", **best_params)\n",
    "        \n",
    "        bayopt_search.fit(x_train,y_train)\n",
    "        results = calculate_scores(bayopt_search, best_params)\n",
    "                \n",
    "        return(results)\n",
    "    \n",
    "    # if tuning method is TPOT\n",
    "    elif opt_type == 'tpot':\n",
    "        print(\"Optimizing hyperparameters with TPOT...\")\n",
    "        \n",
    "        # specify config for regressor\n",
    "        if isinstance(model, RandomForestRegressor):\n",
    "            tpot_config = {\n",
    "                'sklearn.ensemble.RandomForestRegressor': {\n",
    "                    \"n_estimators\": n_estimators,\n",
    "                    \"max_features\": max_features,\n",
    "                    \"min_samples_split\": min_samples_split,\n",
    "                    \"min_samples_leaf\": min_samples_leaf,\n",
    "                    \"bootstrap\": bootstrap\n",
    "                }\n",
    "            }\n",
    "\n",
    "            tpot = TPOTRegressor(generations=5,\n",
    "                                 scoring=\"r2\",\n",
    "                                 population_size=15,\n",
    "                                 verbosity=2,\n",
    "                                 config_dict=tpot_config,\n",
    "                                 cv=cv,\n",
    "                                 random_state=8675309)\n",
    "\n",
    "            tpot.fit(x_train, y_train)\n",
    "            tpot.export('tpot_rf.py')\n",
    "\n",
    "            # process output file to get params\n",
    "            with open(\"tpot_rf.py\", \"r\") as fp:\n",
    "                for line in lines_that_start_with(\"exported_pipeline = \", fp):\n",
    "                    parse_this = line\n",
    "\n",
    "            p = re.compile(r\"[\\w]+=[\\w|[\\d+\\.\\d]+\")\n",
    "            match_list = p.findall(parse_this)\n",
    "            best_params = {}\n",
    "\n",
    "            for match in match_list:\n",
    "                key, val = match.split(\"=\")\n",
    "                best_params[key] = eval(val)\n",
    "\n",
    "            results = calculate_scores(tpot, best_params)\n",
    "\n",
    "        # specify config for XGB\n",
    "        else:\n",
    "            tpot_config = {\n",
    "                'xgboost.XGBClassifier': {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'subsample': subsample,\n",
    "                    'min_child_weight': min_child_weight,\n",
    "                    'reg_alpha': reg_alpha,\n",
    "                    'reg_lambda': reg_lambda,\n",
    "                    'nthread': [1],\n",
    "                    'objective': ['binary:logistic'],\n",
    "                }\n",
    "            }\n",
    "\n",
    "            tpot = TPOTClassifier(generations=5,\n",
    "                     population_size=1,\n",
    "                     verbosity=2,\n",
    "                     config_dict=tpot_config,\n",
    "                     cv=cv,\n",
    "                     random_state=8675309)\n",
    "\n",
    "            tpot.fit(x_train, y_train)\n",
    "            tpot.export('tpot_xbg.py')\n",
    "            \n",
    "            # process output file to get params\n",
    "            with open(\"tpot_xbg.py\", \"r\") as fp:\n",
    "                # for line in lines_that_contain(\"exported_pipeline =\", fp):\n",
    "                #     parse_this = line\n",
    "\n",
    "                for line in lines_that_contain(\"XGB\", fp):\n",
    "                    parse_this = line\n",
    "\n",
    "            p = re.compile(r\"[\\w]+=[\\d+\\.\\d+]+\")\n",
    "            match_list = p.findall(parse_this)\n",
    "            best_params = {}\n",
    "\n",
    "            for match in match_list:\n",
    "                key, val = match.split(\"=\")\n",
    "                best_params[key] = eval(val)\n",
    "\n",
    "            if 'nthread' in best_params:\n",
    "                del best_params['nthread']\n",
    "\n",
    "            results = calculate_scores(tpot, best_params)\n",
    "        return(results)\n",
    "\n",
    "def wrapper(model, cv = 3, n = 3):\n",
    "    start_time = timeit.default_timer()\n",
    "    tuning_methods = ['gridcv','randomcv','bayes','tpot']\n",
    "    \n",
    "    if isinstance(model, RandomForestRegressor):\n",
    "        cols = ['n_estimators','max_features','min_samples_split','min_samples_leaf','bootstrap','rmse','r_squared']\n",
    "    else:\n",
    "        cols = ['n_estimators','max_depth','min_child_weight','learning_rate','subsample','reg_lambda','reg_alpha','accuracy','precision','sensitivity']\n",
    "    \n",
    "    df  = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    print(\"Parallel processing being used for all tuning methods except TPOT\")\n",
    "    for method in tqdm_notebook(tuning_methods):\n",
    "        results = optimize_model(model, opt_type = method, cv = cv, n = n)\n",
    "        df.loc[method] = results\n",
    "        \n",
    "    stop_time = timeit.default_timer()\n",
    "        \n",
    "    print(f\"Done! Time elapsed: {round(stop_time - start_time)} seconds\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing being used for all tuning methods except TPOT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9003193c9ba348eb947f59fa439fb094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters with GridSearchCV...\n",
      "Optimizing hyperparameters with RandomSearchCV...\n",
      "Optimizing hyperparameters with Bayesian Optimization...\n",
      "Optimizing hyperparameters with TPOT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=90, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.43028051129919564\n",
      "Generation 2 - Current best internal CV score: 0.4333338187237798\n",
      "Generation 3 - Current best internal CV score: 0.4333338187237798\n",
      "Generation 4 - Current best internal CV score: 0.43703229676507416\n",
      "Generation 5 - Current best internal CV score: 0.43703229676507416\n",
      "\n",
      "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.525, min_samples_leaf=1, min_samples_split=11, n_estimators=150)\n",
      "\n",
      "Done! Time elapsed: 64 seconds\n"
     ]
    }
   ],
   "source": [
    "# define model and pass it to wrapper\n",
    "rf_model = RandomForestRegressor(random_state=0)\n",
    "# optimize_model(rf_model, opt_type=\"tpot\", cv=2, n=3)\n",
    "df = wrapper(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>gridcv</td>\n",
       "      <td>80</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>53.462674</td>\n",
       "      <td>0.546320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>randomcv</td>\n",
       "      <td>131</td>\n",
       "      <td>0.632398</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>51.525567</td>\n",
       "      <td>0.578601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bayes</td>\n",
       "      <td>150</td>\n",
       "      <td>0.577421</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>53.692206</td>\n",
       "      <td>0.542416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tpot</td>\n",
       "      <td>150</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>53.552177</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n_estimators  max_features min_samples_split min_samples_leaf  \\\n",
       "gridcv             80      0.525000                11                1   \n",
       "randomcv          131      0.632398                 2                5   \n",
       "bayes             150      0.577421                 2               20   \n",
       "tpot              150      0.525000                11                1   \n",
       "\n",
       "         bootstrap       rmse  r_squared  \n",
       "gridcv        True  53.462674   0.546320  \n",
       "randomcv      True  51.525567   0.578601  \n",
       "bayes         True  53.692206   0.542416  \n",
       "tpot          True  53.552177   0.544800  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "<font color = \"blue\"> *** 5 points: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran this model for much longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Optimize XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimized hyperparameters for an xgboost classifier model. \n",
    "\n",
    "This problem contains 5 parts.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "\n",
    "#### About the data\n",
    "The first cell below loads a subset of the loans default data from DS705 and your job is to predict whether a loan defaults or not.  The `status_bad` column is the target column and a 1 indicates a loan that defaulted.  We have selected a subset of the original data that includes 2000 each of good and bad loans.  The data has already been cleaned and encoded.  You're welcome to look into a different dataset, but start by getting this working and then add your own data.\n",
    "\n",
    "#### This is classification, not regression\n",
    "The score for each model will be accuracy and not MSE.  Your summary table should include accuracy, sensitivity, and precision for each optimized model applied to the test data.  (<a href=\"https://classeval.wordpress.com/introduction/basic-evaluation-measures/\">Here is a nice overview of metrics for binary classification data</a>) that includes definitions of accuracy and such.\n",
    "\n",
    "For the models you'll mostly just need to change 'regressor' to 'classifier', e.g. `XGBClassifier` instead of `XGBRegressor`.\n",
    "\n",
    "\n",
    "Hyperparameter | Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 50 to 150\n",
    "max_depth | discrete / integer | 3| 1 to 10\n",
    "min_child_weight | discrete / integer | 1 | 1 to 20\n",
    "learning_rate | continuous / float | 0.1 | 0.001 to 1\n",
    "sub_sample | continuous / float | 1 | 0.05 to 1\n",
    "reg_lambda | continuous / float | 1 | 0 to 5\n",
    "reg_alpha  | continuous / float | 0 | 0 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell for loading and preparing the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv('./data/loans_subset.csv')\n",
    "\n",
    "# split into predictors and target\n",
    "# convert to numpy arrays for xgboost, OK for other models too\n",
    "y = np.array(X['status_Bad']) # 1 for bad loan, 0 for good loan\n",
    "x = np.array(X.drop(columns = ['status_Bad']))\n",
    "\n",
    "# split into test and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0) # notice the lower case x/y labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Write a function called `my_classifier_results` modeled after `my_regression_results` that applies a model to the test data and prints out the accuracy, sensitivity, precision, and the confusion matrix.  There is no need to make a plot.\n",
    "\n",
    "<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def my_classifier_results(model, x_test = x_test, y_test = y_test):\n",
    "    y_pred = model.predict(x_test)    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    sensitivity = recall_score(y_test, y_pred, average=\"weighted\")    \n",
    "    print(f\"Accuracy: {accuracy}, precision: {round(precision,4)}, sensitivity: {round(sensitivity,4)}\\n\")\n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred, labels=[1,0]), \n",
    "        index=['true:bad', 'true:good'], \n",
    "        columns=['pred:bad','pred:good']\n",
    "    )\n",
    "    print(f\"{cmtx}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Start by training some baseline models using default values of the hyperparameters.  We've included logistic regression in a cell below to get you started.  Use `LogisticRegression`, `RandomForestClassifier`, and `GaussianNB` (Gaussian Naive Bayes) from `sklearn`.  Also use `XGBClassifier` from `xgboost` where you may need to include `objective=\"binary:logistic\"` as an option. The default scoring method for all of the `sklearn` classifiers is accuracy. Apply `my_classifier_results` to the test data for each model.\n",
    "\n",
    "<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting RandomForest classifier\n",
      "\n",
      "Accuracy: 0.6575, precision: 0.6584, sensitivity: 0.6575\n",
      "\n",
      "           pred:bad  pred:good\n",
      "true:bad        119         78\n",
      "true:good        59        144\n",
      "\n",
      "Fitting LogisticRegression classifier\n",
      "\n",
      "Accuracy: 0.5475, precision: 0.5507, sensitivity: 0.5475\n",
      "\n",
      "           pred:bad  pred:good\n",
      "true:bad        126         71\n",
      "true:good       110         93\n",
      "\n",
      "Fitting GaussianBN classifier\n",
      "\n",
      "Accuracy: 0.56, precision: 0.5851, sensitivity: 0.56\n",
      "\n",
      "           pred:bad  pred:good\n",
      "true:bad        160         37\n",
      "true:good       139         64\n",
      "\n",
      "Fitting XGB classifier\n",
      "\n",
      "Accuracy: 0.6625, precision: 0.6627, sensitivity: 0.6625\n",
      "\n",
      "           pred:bad  pred:good\n",
      "true:bad        132         65\n",
      "true:good        70        133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "log_classifier = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "gnb_classifier = GaussianNB()\n",
    "xgbr_classifier = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "model_names = [\"RandomForest classifier\",\"LogisticRegression classifier\", \"GaussianBN classifier\", \"XGB classifier\"]\n",
    "models = [rf_classifier, log_classifier, gnb_classifier, xgbr_classifier]\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    print(f\"Fitting {name}\\n\")\n",
    "    model.fit(x_train, y_train)\n",
    "    my_classifier_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "Now use the four hyperparameter optimization techniques on `XGBClassifier` and TPOT general model optimization.  Apply `my_classifer_results` to the test data in each case.\n",
    "* Feel free to use 3 folds instead of 5 for cross validation to speed things up. \n",
    "* Choose a very small number of iterations, population size, etc. until you're sure things are working correctly, then turn up the numbers.  General TPOT optimization will take a while (fair warning: it took about 30 minutes on my Macbook Pro with generations = 10, population_size=40, and cv=5)  \n",
    "* The hyperparameters to consider for are the same as they were in the presentation , but here they are again for convenience:\n",
    "\n",
    "<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   42.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1], 'max_depth': [2, 4, 6],\n",
       "                         'min_child_weight': [1, 3], 'n_estimators': [10, 100],\n",
       "                         'reg_alpha:': [1, 3], 'reg_lambda': [1, 3],\n",
       "                         'subsample': [0.8, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the grid\n",
    "params = {\n",
    "    \"learning_rate\": [0.01, 0.1],\n",
    "    \"max_depth\": [2, 4, 6],\n",
    "    \"n_estimators\": [10, 100],\n",
    "    \"subsample\": [0.8, 1],\n",
    "    \"min_child_weight\": [1, 3],\n",
    "    \"reg_lambda\": [1, 3],\n",
    "    \"reg_alpha:\": [1, 3]\n",
    "}\n",
    "\n",
    "# setup the grid search\n",
    "grid_search = GridSearchCV(xgbr_classifier,\n",
    "                           param_grid=params,\n",
    "                           cv=3,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing being used for all tuning methods except TPOT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05034d6fe2e2442ea396dc3391a64621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters with GridSearchCV...\n",
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   52.1s\n",
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters with RandomSearchCV...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters with Bayesian Optimization...\n",
      "Optimizing hyperparameters with TPOT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=6, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.4997222222222222\n",
      "Generation 2 - Current best internal CV score: 0.4997222222222222\n",
      "Generation 3 - Current best internal CV score: 0.4997222222222222\n",
      "Generation 4 - Current best internal CV score: 0.6483333333333333\n",
      "Generation 5 - Current best internal CV score: 0.6483333333333333\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=0.001, max_depth=1, min_child_weight=20, n_estimators=150, nthread=1, objective=binary:logistic, reg_alpha=1, reg_lambda=5, subsample=1.0)\n",
      "\n",
      "Done! Time elapsed: 155 seconds\n"
     ]
    }
   ],
   "source": [
    "# define model and pass it to wrapper\n",
    "# optimize_model(xgbr_classifier, opt_type=\"gridcv\", cv=3, n=2)\n",
    "df = wrapper(xgbr_classifier, cv=3, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>gridcv</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.6300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>randomcv</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.183223</td>\n",
       "      <td>0.786292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6025</td>\n",
       "      <td>0.602415</td>\n",
       "      <td>0.6025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bayes</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.105238</td>\n",
       "      <td>0.778466</td>\n",
       "      <td>1.214367</td>\n",
       "      <td>1.691005</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.657458</td>\n",
       "      <td>0.6575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tpot</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.635929</td>\n",
       "      <td>0.6300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          n_estimators  max_depth  min_child_weight  learning_rate  subsample  \\\n",
       "gridcv            50.0        1.0               1.0       1.000000   1.000000   \n",
       "randomcv          70.0        8.0               5.0       0.183223   0.786292   \n",
       "bayes            150.0        1.0               1.0       0.105238   0.778466   \n",
       "tpot             150.0        1.0              20.0       0.001000   1.000000   \n",
       "\n",
       "          reg_lambda  reg_alpha  accuracy  precision  sensitivity  \n",
       "gridcv      0.000000        NaN    0.6300   0.630163       0.6300  \n",
       "randomcv    0.000000        NaN    0.6025   0.602415       0.6025  \n",
       "bayes       1.214367   1.691005    0.6575   0.657458       0.6575  \n",
       "tpot        5.000000   1.000000    0.6300   0.635929       0.6300  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit TPOT AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=30, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.655833381430056\n",
      "Generation 2 - Current best internal CV score: 0.655833381430056\n",
      "Generation 3 - Current best internal CV score: 0.6566668693416263\n",
      "Generation 4 - Current best internal CV score: 0.6566668693416263\n",
      "Generation 5 - Current best internal CV score: 0.6616674881689778\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.01, max_depth=5, max_features=0.55, min_samples_leaf=15, min_samples_split=7, n_estimators=100, subsample=0.2)\n",
      "0.655\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5,\n",
    "                     population_size=5,\n",
    "                     verbosity=2,\n",
    "                     cv=2,\n",
    "                     random_state=8675309)\n",
    "\n",
    "tpot.fit(x_train, y_train)\n",
    "print(tpot.score(x_test, y_test))\n",
    "tpot.export('tpot_optimal_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655, precision: 0.6567, sensitivity: 0.655\n",
      "\n",
      "           pred:bad  pred:good\n",
      "true:bad        137         60\n",
      "true:good        78        125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_classifier_results(tpot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Summary\n",
    "\n",
    "* In addition to your summary table, answer:\n",
    "    * If the bank is primarily interested in correctly identifying loans that are truly bad, then which model should they use?  Why?\n",
    "\n",
    "<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
