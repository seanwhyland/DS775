{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":79437824},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"294.56px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"3da46a","input":"# execute to import notebook styling for tables and width etc.\nfrom IPython.core.display import HTML\nimport urllib.request\nresponse = urllib.request.urlopen('https://raw.githubusercontent.com/DataScienceUWL/DS775v2/master/ds755.css')\nHTML(response.read().decode(\"utf-8\"));","metadata":{"code_folding":[0]},"pos":1,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"75e497","input":"# execute this cell for video\nfrom IPython.display import IFrame\nIFrame(\n    \"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-regression/index.html\",\n    width=640,\n    height=360)","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"\n        <iframe\n            width=\"640\"\n            height=\"360\"\n            src=\"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-regression/index.html\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ","text/plain":"<IPython.lib.display.IFrame at 0x10c11bdd8>"},"exec_count":1,"output_type":"execute_result"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"ca0825","input":"def rastrigin(x):\n    # pass a single vector of length n (=dim) to evaluate Rastrigin\n    return sum(x**2 + 10 - 10 * np.cos(2 * np.pi * x))\n\ndim = 10\nnum_local_searches = 1000\nbest_value = 1.e10\n\nfor i in range(num_local_searches):\n    x_initial = np.random.uniform(-5.12, 5.12, dim)\n    result = minimize(rastrigin, x_initial)\n    if result.fun < best_value:\n        best_value = result.fun\n        best_x = result.x\n\nprint(\n    'The smallest value find is {:4.3f} at x = {:1.3f} and y = {:1.3f}'.format(\n        best_value, best_x[0], best_x[1]))","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"The smallest value find is 20.894 at x = -0.000 and y = 2.985\n"}},"pos":55,"type":"cell"}
{"cell_type":"code","exec_count":115,"id":"958cb0","input":"print('The minimum value is {:0.4f} and occurs at x = {:0.4f}'.format(result.fun,result.x[0]))","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"The minimum value is 0.5625 and occurs at x = -0.5000\n"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":118,"id":"0a5384","input":"# add your code here, note you should be able to guess an initial value from the graph ...","metadata":{"hidden":true},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"b1af55","input":"# 2-opt local search for TSP\n\ndef two_opt(xy):\n    num_cities = xy.shape[0]\n    current_tour = np.random.permutation(np.arange(num_cities))\n    current_dist = tour_dist(current_tour, xy)\n    best_tour = current_tour\n    best_dist = current_dist\n\n    improvement = True\n    iterations = 0\n    while improvement:\n        improvement = False\n        for i in range(num_cities - 1):\n            for j in range(i + 1, num_cities):\n                iterations += 1\n                new_tour = sub_tour_reversal(best_tour, i, j)\n                new_dist = tour_dist(new_tour, xy)\n                if new_dist < best_dist:\n                    best_tour = new_tour\n                    best_dist = new_dist\n                    improvement = True\n    return best_tour, best_dist, iterations\n\nbest_tour, best_dist, iterations = two_opt(xy)\n                \nplot_tour(best_tour,xy,best_dist)\nprint('The minimum distance found is {:d} after {:d} iterations'.format(int(best_dist),iterations))","metadata":{"hidden":true},"output":{"0":{"more_output":true}},"pos":86,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"0a3318","input":"# imports\n%matplotlib notebook\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.optimize import minimize\nimport babel.numbers as numbers\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nimport json\nimport time\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom mpl_toolkits.basemap import Basemap","metadata":{"code_folding":[0]},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"945e14","input":"# execute this cell for video\nfrom IPython.display import IFrame\nIFrame(\n    \"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-traveling-salesman-problem/index.html\",\n    width=640,\n    height=360)","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"\n        <iframe\n            width=\"640\"\n            height=\"360\"\n            src=\"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-logistic-traveling-salesman-problem/index.html\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ","text/plain":"<IPython.lib.display.IFrame at 0x10c4b8668>"},"exec_count":2,"output_type":"execute_result"}},"pos":74,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"ca529d","input":"# imports\nimport pandas as pd \nimport numpy as np","metadata":{"code_folding":[0],"hidden":true},"pos":93,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"4c9dc3","input":"# load the data + random assignment\nnum_districts = 10\nmin_voters_in_district = 150\nmax_voters_in_district = 350\n\ndems = [152,81,75,34,62,38,48,74,98,66,83,86,72,28,112,45,93,72]\nreps = [62,59,83,52,87,87,69,49,62,72,75,82,83,53,98,82,68,98]\ncities = pd.DataFrame( data = {'dems':dems, 'reps':reps})\n\n# assign = np.random.randint(low=0,high=num_districts,size = 18)\nassign = np.array([4, 3, 1, 9, 4, 0, 2, 8, 0, 9, 8, 0, 0, 0, 8, 7, 3, 6])\nassign","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"data":{"text/plain":"array([4, 3, 1, 9, 4, 0, 2, 8, 0, 9, 8, 0, 0, 0, 8, 7, 3, 6])"},"exec_count":24,"output_type":"execute_result"}},"pos":95,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"11ca31","input":"def summarize_districts(assign, cities):\n    reps = np.zeros(num_districts, dtype=np.int32)\n    dems = np.zeros(num_districts, dtype=np.int32)\n    df = cities.groupby(assign).sum()\n    reps[df.index] = df['reps']\n    dems[df.index] = df['dems']\n    total = reps + dems\n    delta = np.minimum(np.maximum(total, min_voters_in_district),\n                       max_voters_in_district) - total\n    rep_win = reps > dems\n    dict = {\n        'reps': reps,\n        'dems': dems,\n        'total': total,\n        'rep_win': rep_win\n    }\n    return (pd.DataFrame(data=dict))\n\nsummarize_districts(assign, cities)","metadata":{"hidden":true},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reps</th>\n      <th>dems</th>\n      <th>total</th>\n      <th>rep_win</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>367</td>\n      <td>322</td>\n      <td>689</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>83</td>\n      <td>75</td>\n      <td>158</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>69</td>\n      <td>48</td>\n      <td>117</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>127</td>\n      <td>174</td>\n      <td>301</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>149</td>\n      <td>214</td>\n      <td>363</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>98</td>\n      <td>72</td>\n      <td>170</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>82</td>\n      <td>45</td>\n      <td>127</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>222</td>\n      <td>269</td>\n      <td>491</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>124</td>\n      <td>100</td>\n      <td>224</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   reps  dems  total  rep_win\n0   367   322    689     True\n1    83    75    158     True\n2    69    48    117     True\n3   127   174    301    False\n4   149   214    363    False\n5     0     0      0    False\n6    98    72    170     True\n7    82    45    127     True\n8   222   269    491    False\n9   124   100    224     True"},"exec_count":27,"output_type":"execute_result"}},"pos":97,"type":"cell"}
{"cell_type":"code","exec_count":28,"id":"028995","input":"def fitness_districts(assign, cities):\n    df = cities.groupby(assign).sum()\n    fitness = sum( df['reps'] > df['dems'] )\n    total_voters = np.zeros(num_districts,dtype=np.int32)\n    total_voters[df.index] = df.sum(axis=1)\n    fitness -= np.abs(np.minimum(np.maximum(total_voters,150),350)-total_voters).sum()\n    return (fitness)\n\nfitness_districts(assign,cities)","metadata":{"hidden":true},"output":{"0":{"data":{"text/plain":"-693"},"exec_count":28,"output_type":"execute_result"}},"pos":99,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"6ea5b5","input":"# load the data and define move and objective functions\nmap = Basemap(llcrnrlon=-119,\n              llcrnrlat=22,\n              urcrnrlon=-64,\n              urcrnrlat=49,\n              projection='lcc',\n              lat_1=32,\n              lat_2=45,\n              lon_0=-95)\n\n# read 48 capitals lat and lon\nwith open('./data/capitals48.json', 'r') as json_file:\n    capitals = json.load(json_file)\n\n# loop through the capitals to store x,y coordinates (meters)\nnum_cities = len(capitals)\nxy = np.zeros((num_cities, 2))\nfor i in range(num_cities):\n    lat = capitals[i][\"lat\"]\n    lon = capitals[i][\"long\"]\n    xy[i, :] = map(lon, lat)\n\ndef plot_tour(best_tour, xy, best_dist):\n    fig = plt.figure()\n    fig.set_size_inches(6, 4)\n\n    # load the shape file with \"states\"\n    map.readshapefile('./data/st99_d00', name='states', drawbounds=True)\n\n    loop_tour = np.append(best_tour, best_tour[0])\n    map.plot(xy[:, 0], xy[:, 1], c='r', marker='o', markersize=4, linestyle='')\n    lines, = map.plot(xy[loop_tour, 0],\n                      xy[loop_tour, 1],\n                      c='b',\n                      linewidth=1,\n                      linestyle='-')\n    plt.title('Best Distance {:d} km'.format(int(best_dist)))","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"name":"stderr","output_type":"stream","text":"/Users/jbaggett/anaconda3/envs/ds775/lib/python3.7/site-packages/ipykernel_launcher.py:9: MatplotlibDeprecationWarning: \nThe dedent function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use inspect.cleandoc instead.\n  if __name__ == '__main__':\n"}},"pos":77,"type":"cell"}
{"cell_type":"code","exec_count":31,"id":"d28531","input":"# execute first to initialize the plot\nfig = plt.figure()\nfig.set_size_inches(6,4)\nfig.canvas.draw()\nfig.show()","metadata":{"code_folding":[0],"hidden":true},"output":{"0":{"more_output":true}},"pos":82,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"577275","input":"# open to reveal graph code\ndef rastrigin_1D(x):\n    return (x**2 + 10 - 10 * np.cos(2 * np.pi * x))\n    \nx = np.linspace(-5.12,5.12,201)\ny = rastrigin_1D(x)\n\nfig = plt.figure(figsize=(3,3))\nplt.plot(x,y)\nplt.xlabel('x');\nplt.ylabel('y');","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"more_output":true}},"pos":52,"type":"cell"}
{"cell_type":"code","exec_count":34,"id":"a89877","input":"# graph of data and sigmoid\nb0 = -4.07771657\nb1 = 1.5046468\nhours_studied = np.linspace(0,6,101)\ndef sigmoid(x,intercept,slope):\n    return( 1.0 / (1.0 + np.exp( -(intercept + slope * x) ) ) )\nprob_passed = sigmoid(hours_studied, b0, b1)\n\nfig = plt.figure();\nfig.set_size_inches(6,3.5);\nax = fig.add_subplot(111);\nax.scatter(x_hours, y_passed);\nax.plot(hours_studied, prob_passed);\nax.set_xlabel('hours studied');\nax.set_ylabel('passed');","metadata":{"code_folding":[0],"hidden":true},"output":{"0":{"more_output":true}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":35,"id":"9e6b1d","input":"print(\n    'A student who studies 4 hours has approximately {:3.1f}% chance of passing.'\n    .format(100 * sigmoid(4, b0, b1)))","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"A student who studies 4 hours has approximately 87.4% chance of passing.\n"}},"pos":41,"type":"cell"}
{"cell_type":"code","exec_count":38,"id":"7319bf","input":"# 3D Graph of Rastrigin with dimension n = 2\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport numpy as np\n\nx = np.linspace(-5.12, 5.12, 401)     \ny = np.linspace(-5.12, 5.12, 401)     \nX, Y = np.meshgrid(x, y) \nZ = (X**2 - 10 * np.cos(2 * np.pi * X)) + \\\n  (Y**2 - 10 * np.cos(2 * np.pi * Y)) + 20\n\ndata = [\n    go.Surface( x = X, y = Y, z = Z, colorscale = 'Jet',\n        contours=go.surface.Contours(\n            z=go.surface.contours.Z(\n              show=True,\n              usecolormap=True,\n              highlightcolor=\"#42f462\",\n              project=dict(z=True)\n            )\n        )\n    )\n]\n\nlayout = go.Layout(title='Rastrigin',width=600,height=600)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","metadata":{"code_folding":[0],"hidden":true},"output":{"0":{"more_output":true}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"58e905","input":"# define move and objective functions\n\ndef sub_tour_reversal(tour):\n    n = len(tour)\n    i, j = np.sort(np.random.choice(n, 2, replace=False))\n    return (np.concatenate((tour[0:i], tour[j:-n + i - 1:-1], tour[j + 1:n])))\n\ndef tour_dist(tour, xy):\n    # returns tour distance in kilometers\n    xy_tmp = xy[tour, :]\n    return (np.sum(\n        np.sqrt(\n            np.sum(np.diff(xy_tmp, axis=0, append=[xy_tmp[0, :]])**2, axis=1)))\n            / 1000)","metadata":{"code_folding":[],"hidden":true},"pos":79,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"f1171f","input":"# unfold to see Pyomo solution for Wyndor Quadratic Program\nimport pyomo.environ as pyo\n\n# Concrete Model\nmodel = pyo.ConcreteModel(name=\"Wyndor\")\n\nproducts = ['drs', 'wdw']\n\nbounds_dict = {'drs': (0, 4), 'wdw': (0, 6)}\n\n\ndef bounds_rule(model, product):\n    return (bounds_dict[product])\n\n\nmodel.x = pyo.Var(products, domain=pyo.Reals, bounds=bounds_rule)\n\n# Objective\nmodel.profit = pyo.Objective(expr=126.0 * model.x['drs'] -\n                         9.0 * model.x['drs']**2 + 182.0 * model.x['wdw'] -\n                         13.0 * model.x['wdw']**2.0,\n                         sense=pyo.maximize)\n\n# Constraints\nmodel.Constraint3 = pyo.Constraint(\n    expr=3.0 * model.x['drs'] + 2.0 * model.x['wdw'] <= 18)\n\n# Solve\nsolver = pyo.SolverFactory('ipopt')\nsolver.solve(model)\n\n# display(model)\n\n# display solution\nimport babel.numbers as numbers  # needed to display as currency\nprint(\"Profit = \",\n      numbers.format_currency(1000 * model.profit(), 'USD', locale='en_US'))\nprint(\"Batches of Doors = {:1.2f}\".format(model.x['drs']()))\nprint(\"Batches of Windows = {:1.2f}\".format(model.x['wdw']()))","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"Profit =  $857,000.00\nBatches of Doors = 2.67\nBatches of Windows = 5.00\n"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":47,"id":"9ed9f5","input":"from sklearn.linear_model import LogisticRegression    \nmodel = LogisticRegression(C=1.0e10,fit_intercept = True)\nmodel.fit(x_hours.reshape(-1,1), y_passed)\nb0 = model.intercept_[0]\nb1 = model.coef_[0][0]\nprint('The maximum likelihood estimate for p(x) has intercept b0 = {:2.3f} and slope b1 = {:2.3f}'.format(b0,b1))","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"The maximum likelihood estimate for p(x) has intercept b0 = -4.078 and slope b1 = 1.505\n"}},"pos":49,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"807f8b","input":"# data from \nx_hours = np.array([\n    0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 1.75, 2.0, 2.25, 2.50, 2.75, 3.00, 3.25,\n    3.5, 4.0, 4.25, 4.5, 4.75, 5.0, 5.5\n])\ny_passed = np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])","metadata":{"hidden":true},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"202a77","input":"def neg_log_loss( coef, *args):\n    b0 = coef[0]\n    b1 = coef[1]\n    x = args[0]\n    y = args[1]\n    p = 1.0/(1.0 + np.exp(-(b0 + b1*x)))\n    ll = sum( y*np.log(p)+(1-y)*np.log(1-p) )\n    return(-ll) # here's the minus sign!","metadata":{"hidden":true},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"928108","input":"# local search for TSP solution with a random segment reversal at each iteration\n\ndef random_reversals(xy, max_no_improve):\n    num_cities = xy.shape[0]\n    # starts from a random tour\n    current_tour = np.random.permutation(np.arange(num_cities))\n    current_dist = tour_dist(current_tour, xy)\n    best_tour = current_tour\n    best_dist = current_dist\n\n    # stop search if no better tour is found within max_no_improve iterations, can increase to eliminate crossovers\n    num_moves_no_improve = 0\n    iterations = 0\n    while (num_moves_no_improve < max_no_improve):\n        num_moves_no_improve += 1\n        iterations += 1  # just for tracking\n        new_tour = sub_tour_reversal(current_tour)\n        new_dist = tour_dist(new_tour, xy)\n        if new_dist < current_dist:\n            num_moves_no_improve = 0\n            current_tour = new_tour\n            current_dist = new_dist\n            if current_dist < best_dist:  # not really needed since current_tour will be best\n                best_tour = current_tour  # but we'll use this in the next lesson\n                best_dist = current_dist\n    return best_tour, best_dist, iterations\n\n\nbest_tour, best_dist, iterations = random_reversals(xy, 1000)\nplot_tour(best_tour, xy, best_dist)\nprint('The minimum distance found is {:d} after {:d} iterations'.format(\n    int(best_dist), iterations))","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"more_output":true}},"pos":80,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"640271","input":"result = minimize(neg_log_loss,[0,0],args=(x_hours,y_passed))\nresult","metadata":{"hidden":true},"output":{"0":{"data":{"text/plain":"      fun: 8.029878464344682\n hess_inv: array([[ 3.05194929, -1.02536936],\n       [-1.02536936,  0.39359904]])\n      jac: array([3.57627869e-07, 4.76837158e-07])\n  message: 'Optimization terminated successfully.'\n     nfev: 56\n      nit: 12\n     njev: 14\n   status: 0\n  success: True\n        x: array([-4.07771322,  1.50464537])"},"exec_count":8,"output_type":"execute_result"}},"pos":46,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"6578f7","input":"# execute to watch the progress of random reversals local search\n\nmap.readshapefile('./data/st99_d00', name='states', drawbounds=True)\n\n# initialize with a random tour\nn = 48\nbest_tour = np.random.permutation(np.arange(n))\nbest_dist = tour_dist(best_tour, xy)/1000\n\n# plot initial tour\nloop_tour = np.append(best_tour, best_tour[0])\nmap.plot(xy[:, 0], xy[:, 1], c='r', marker='o', markersize=4, linestyle='')\nlines, = map.plot(xy[loop_tour, 0],\n                  xy[loop_tour, 1],\n                  c='b',\n                  linewidth=1,\n                  linestyle='-')\nfig.canvas.draw()\nplt.show()\n\ncount = 1\niteration = np.array([count])\ndistances = np.array([best_dist])\ndst_label = plt.text(250000, 650000, '{:d} km'.format(int(best_dist)))\n\n\n# initialize with a random tour\ncurrent_tour = np.random.permutation(np.arange(num_cities))\ncurrent_dist = tour_dist(current_tour, xy)\nbest_tour = current_tour\nbest_dist = current_dist\n\nmax_moves_no_improve = 5000\nnum_moves_no_improve = 0\nwhile( num_moves_no_improve < max_moves_no_improve):\n    num_moves_no_improve += 1\n    new_tour = sub_tour_reversal(current_tour)\n    new_dist = tour_dist(new_tour, xy)/1000\n    if new_dist < current_dist:\n        current_tour = new_tour\n        current_dist = new_dist\n        num_moves_no_improve = 0\n        if current_dist < best_dist: # not really needed since current_tour will be best\n            best_tour = current_tour # but we'll use this in the next lesson\n            best_dist = current_dist\n        loop_tour = np.append(best_tour, best_tour[0])\n        lines.set_data(xy[loop_tour, 0], xy[loop_tour, 1])\n        dst_label.set_text('{:d} km'.format(int(best_dist)))   \n        fig.canvas.draw()\n        plt.show()  \n","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"name":"stderr","output_type":"stream","text":"/Users/jbaggett/anaconda3/envs/ds775/lib/python3.7/site-packages/ipykernel_launcher.py:3: MatplotlibDeprecationWarning: \nThe dedent function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use inspect.cleandoc instead.\n  This is separate from the ipykernel package so we can avoid doing imports until\n"},"1":{"ename":"NameError","evalue":"name 'fig' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-7957a68f5a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                   \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                   linestyle='-')\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'fig' is not defined"]}},"pos":83,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"e3ebd3","input":"b0 = result.x[0]\nb1 = result.x[1]\nprint('The maximum likelihood estimate for p(x) has intercept b0 = {:2.3f} and slope b1 = {:2.3f}'.format(b0,b1))","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"The maximum likelihood estimate for p(x) has intercept b0 = -4.078 and slope b1 = 1.505\n"}},"pos":47,"type":"cell"}
{"cell_type":"code","id":"14b4f9","input":"# create the map and run the 2opt search\nmap = Basemap(llcrnrlon=-119,\n              llcrnrlat=22,\n              urcrnrlon=-64,\n              urcrnrlat=49,\n              projection='lcc',\n              lat_1=32,\n              lat_2=45,\n              lon_0=-95)\n\n# load the shape file with \"states\"\nmap.readshapefile('./data/st99_d00', name='states', drawbounds=True)\n\n# read 48 capitals lat and lon\nwith open('./data/capitals48.json', 'r') as json_file:\n    capitals = json.load(json_file)\nnum_cities = 48\n\n# loop through the capitals to store x,y coordinates (meters)\nxy = np.zeros((num_cities, 2))\nfor i in range(num_cities):\n    lat = capitals[i][\"lat\"]\n    lon = capitals[i][\"long\"]\n    xy[i, :] = map(lon, lat)\n\n\ndef sub_tour_reversal(tour, i, j):\n    # reverse the segment from city i to city j\n    n = len(tour)\n    return (np.concatenate((tour[0:i], tour[j:-n + i - 1:-1], tour[j + 1:n])))\n\n\ndef tour_dist(tour, xy):\n    # euclidean distance returned\n    xy_tmp = xy[tour, :]\n    return (np.sum(\n        np.sqrt(\n            np.sum(np.diff(xy_tmp, axis=0, append=[xy_tmp[0, :]])**2,\n                   axis=1))))\n\n\n# initialize with a random tour\nn = 48\nbest_tour = np.random.permutation(np.arange(n))\nbest_dist = tour_dist(best_tour, xy)/1000\nimprovement = True\n\n# plot initial tour\nloop_tour = np.append(best_tour, best_tour[0])\nmap.plot(xy[:, 0], xy[:, 1], c='r', marker='o', markersize=4, linestyle='')\nlines, = map.plot(xy[loop_tour, 0],\n                  xy[loop_tour, 1],\n                  c='b',\n                  linewidth=1,\n                  linestyle='-')\nfig.canvas.draw()\nplt.show()\n\ncount = 1\niteration = np.array([count])\ndistances = np.array([best_dist])\ndst_label = plt.text(250000, 650000, '{:d} km'.format(int(best_dist)))\n\n\n# initialize with a random tour\ncurrent_tour = np.random.permutation(np.arange(num_cities))\ncurrent_dist = tour_dist(current_tour, xy)\nbest_tour = current_tour\nbest_dist = current_dist\n\nmax_moves_no_improve = 5000\nnum_moves_no_improve = 0\nwhile( num_moves_no_improve < max_moves_no_improve):\n    num_moves_no_improve += 1\n    i,j = np.sort(np.random.choice(n,2,replace=False))\n    new_tour = sub_tour_reversal(current_tour, i, j)\n    new_dist = tour_dist(new_tour, xy)/1000\n    if new_dist < current_dist:\n        current_tour = new_tour\n        current_dist = new_dist\n        num_moves_no_improve = 0\n        if current_dist < best_dist: # not really needed since current_tour will be best\n            best_tour = current_tour # but we'll use this in the next lesson\n            best_dist = current_dist\n        loop_tour = np.append(best_tour, best_tour[0])\n        lines.set_data(xy[loop_tour, 0], xy[loop_tour, 1])\n        dst_label.set_text('{:d} km'.format(int(best_dist)))   \n        fig.canvas.draw()\n        plt.show()  \n","metadata":{"code_folding":[0],"hidden":true},"pos":90,"scrolled":true,"type":"cell"}
{"cell_type":"code","id":"651e2a","input":"# graph of profit function\nx = np.linspace(0,250,201) # 201 points between 0 and 250\nP = lambda x:-0.008*x**2 + 3.1*x - 80 # lambda is for writing one line functions\nfig = plt.figure(figsize=(4,3.5));\nplt.plot(x,P(x));\nplt.xlabel('apartments');\nplt.ylabel('profit (\\$ thousands)');","metadata":{"code_folding":[],"hidden":true},"pos":26,"type":"cell"}
{"cell_type":"code","id":"6a463c","input":"# plot p(x) on [-3,3]\nx = np.linspace(-3,3,201)\np = lambda x:x**4 + 2*x**3 + 3*x**2 + 2*x + 1\nfig = plt.figure(figsize=(4,3.5))\nplt.plot(x,p(x));\nplt.xlabel('x');\nplt.ylabel('y');","metadata":{"code_folding":[0],"hidden":true},"pos":20,"scrolled":true,"type":"cell"}
{"cell_type":"code","id":"7c7857","input":"# execute this cell for video\nfrom IPython.display import IFrame\nIFrame(\n    \"https://media.uwex.edu/content/ds/ds775_r19/ds775_lesson4-optimization-basics/index.html\",\n    width=900,\n    height=600)","metadata":{"code_folding":[0],"hidden":true},"pos":5,"type":"cell"}
{"cell_type":"code","id":"87930b","input":"# execute this cell to see scipy.optimize.minimize documentation\nfrom IPython.display import IFrame\nIFrame(\n    \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\",\n    width=900,\n    height=600)","metadata":{"code_folding":[0],"hidden":true},"pos":14,"scrolled":true,"type":"cell"}
{"cell_type":"code","id":"b2d1ea","input":"# plot p(x) on [-10,10]\nx = np.linspace(-10,10,201)\np = lambda x:x**4 + 2*x**3 + 3*x**2 + 2*x + 1\nfig = plt.figure(figsize=(4,3.5))\nplt.plot(x,p(x));\nplt.xlabel('x');\nplt.ylabel('y');","metadata":{"code_folding":[0],"hidden":true},"pos":18,"type":"cell"}
{"cell_type":"code","id":"cc85b8","input":"# execute this cell before those below to initialize the figure display, important for high-res displays\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom mpl_toolkits.basemap import Basemap\nimport json\nimport numpy as np\nimport time\n\nfig = plt.figure()\nfig.set_size_inches(6,4)\nfig.canvas.draw()\nfig.show()","metadata":{"code_folding":[0],"hidden":true},"pos":89,"scrolled":true,"type":"cell"}
{"cell_type":"code","id":"f866f9","input":"# execute for local search\nresult = minimize(p,-2)\nresult","metadata":{"code_folding":[0],"hidden":true},"pos":22,"type":"cell"}
{"cell_type":"markdown","id":"0300e9","input":"Work your way through the embedded storybook below to learn some basic ideas about optimization.  This material complements the material in the textbook.  The graphs and demos in Slides 9-11 are available in the separate file Storybook_Graphs_04.ipynb.","metadata":{"hidden":true},"pos":4,"type":"cell"}
{"cell_type":"markdown","id":"080e1d","input":"Lets investigate the fourth degree polynomial $$p(x) = x^4 + 2 x^3 + 3 x^2 + 2 x + 1.$$  Let's graph it to get an idea of the behavior. ","metadata":{"hidden":true},"pos":17,"type":"cell"}
{"cell_type":"markdown","id":"095444","input":"### <font color = \"blue\">Self Assessment: How many searches when dim = 10?</font>","metadata":{"hidden":true},"pos":64,"type":"cell"}
{"cell_type":"markdown","id":"105fb5","input":"Do 1000 local search with Rastrigin with dim = 10.  What is the smallest value you find?  How long do you think it would take to find the minimum from randomly chosen initial points like this?  ","metadata":{"hidden":true},"pos":59,"type":"cell"}
{"cell_type":"markdown","id":"16e266","input":"2-opt is generally uses fewer iterations to find a reasonable tour (local minimum) than does our local search with random segment reversals.  In the next lesson we'll try using 2-opt to find starting points for a global search algorithm.","metadata":{"hidden":true},"pos":87,"type":"cell"}
{"cell_type":"markdown","id":"22cefa","input":"Here is pseudo-code for a simple local search.  Many variations are possible, but they often look like this:\n```\n set starting state \n while local_condition \n     select a move \n     if acceptable \n         do the move \n         if new optimum \n             remember it \n endwhile \n ```","metadata":{"hidden":true},"pos":69,"type":"cell"}
{"cell_type":"markdown","id":"2f48f2","input":"We'll use Pyomo to solve the quadratic variation of the Wyndor problem illustrated in Figure 13.6 on page 554.\n\n<img src=\"images/wyndor_quad.png\" width=\"600\">\n\nThis profit function is kind of nonsensical, but it will serve to illustrate a Pyomo solution.  We'll use a concrete model formulation for simplicity.  Note, the Pyomo package `minimize` conflicts with the `minimize` from `scipy.optimize` we're using elsewhere in this notebook, so we'll import `pyomo` a bit differently here than usual.  Alternately we could re-import `minimize` from `scipy.optimize` after we're done with Pyomo.","metadata":{"hidden":true},"pos":9,"type":"cell"}
{"cell_type":"markdown","id":"317919","input":"Though it's not important, if you like you can execute the two cells below to see an animated version of this search:","metadata":{"hidden":true},"pos":81,"type":"cell"}
{"cell_type":"markdown","id":"3a5563","input":"### Find the model with `minimize`","metadata":{"hidden":true},"pos":43,"type":"cell"}
{"cell_type":"markdown","id":"3dc63e","input":"### Local Search with Graphing for TSP","metadata":{"hidden":true},"pos":88,"type":"cell"}
{"cell_type":"markdown","id":"40ec1c","input":"### <font color=\"blue\">Self-Assessment:  How many searches?</font>","metadata":{"hidden":true},"pos":62,"type":"cell"}
{"cell_type":"markdown","id":"4814a4","input":"## TSP Local Search Code","metadata":{"heading_collapsed":true,"hidden":true},"pos":75,"type":"cell"}
{"cell_type":"markdown","id":"496f72","input":"Take a minute to see how this function is structured and perhaps glance at the documentation again.  `coef` is a one-dimensional array with shape (n,) that contains all $n$ optimization variables.  In this case there are two which we assign to $b_0$ and $b_1$.  `*args` is a pointer to tuple `args` that contains any additional parameters that should be passed to the function.  In the case `minimize` will be passing the tuple $(x,y)$ that contains the training data.  We'll pass `args = (x_hours, y_passed)`.","metadata":{"hidden":true},"pos":45,"type":"cell"}
{"cell_type":"markdown","id":"49b294","input":"# Local Search - Discrete Variables","metadata":{"heading_collapsed":true},"pos":66,"type":"cell"}
{"cell_type":"markdown","id":"5966e0","input":"If we are maximizing a function of one variable, $f(x)$, we might choose to use 10 starting points.  For a function of two variables, $g(x,y)$ to get the same search power we would choose 10 points in the $x$ direction and 10 points in the $y$ direction to make a grid of $10^2 = 100$ starting points in the $xy$-plane.  For three variables we need $10^3 = 1000$ points in $xyz$-space.  For a function of $n$ variables we would need $10^n$ starting points.\n\n*The volume of the search space grows exponentially with the number of variables or dimensionality of the problem.*\n\nThis is called the curse of dimensionality.  For high dimensional functions like those that occur in training neural networks and other applications with many local minima it can be very difficult to find the global minima because the volume of the search space grows exponentially with the number of variables.\n\nFor the Rastrigin function to find the global minimum you need an initial starting point in the interval (-0.5,0.5) in each dimension.  The search interval is [-5.12,5.12] in each dimension.  Thus the probability that a single uniformly sampled point in [-5.12,5.12] is $\\frac{1}{10.28} \\approx 0.0973$ (the ratio of the lengths of the two intervals).  The probability of finding the global minimum using local search from a uniformly sampled point in $n$ dimensions is $$\\left( \\frac{1}{10.28} \\right)^n$$.  That means we'd have to, on average, start $10.28^n$ local searches from uniformly sampled points to find the global minimum once.","metadata":{"hidden":true},"pos":61,"type":"cell"}
{"cell_type":"markdown","id":"5a137a","input":"For the Rastrigin function write a while loop that runs until the global minimum value is found ($|\\mbox{best_val}|<0.01$) and track the number of iterations. Use a for loop to repeat this three times until your code is debugged.  After your code is working, repeat the process 100 times and report the average number of searches until the global minimum is found when $n=1,2,3$.  Are these numbers in approximate agreement with with the estimated numbers $10.28^n$ (they very likely won't be all that close, but how is the overall trend)?","metadata":{"hidden":true},"pos":63,"type":"cell"}
{"cell_type":"markdown","id":"5cfaeb","input":"### An example","metadata":{"hidden":true},"pos":36,"type":"cell"}
{"cell_type":"markdown","id":"5e8906","input":"*Note:  It isn't important to understand the details of logistic regression for this class, but the text in this cell gives a bit of background.*\n\nWhere do those values for the slope and intercept come from?  To obtain those we find the values of $b_0$ and $b_1$ that maximize the likelihood function:\n$$ L(b_0,b_1) = \\prod_{i=1}^{n} p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}$$\nwhere $(x_i,y_i)$ are the data pairs for each student and $p(x) = \\displaystyle \\frac{1}{1 + e^{-(b_0 + b_1 x)}}$ is the sigmoid function.  By maximizing the likelihood function we are maximizing the probability that this model produced the observed data.  Note that the $\\prod$ symbol means to take the product of the values, like $\\sum$ means to take the sum.\n\nIn practice, maximizing a product can lead to numerical difficulties, so we instead maximize the log-likelihood function found by taking the logarithm of $L(b_0,b_1)$ to get:\n$$LL(b_0, b_1) = \\sum_{i = 1}^{n} \\left[ y_i \\log( p(x_i) ) + (1-y_i) \\log(1-p(x_i)) \\right].$$\n\nNow we need to find $b_0$ and $b_1$ to maximize this.  The log-likelihood function turns out to be concave so that ascending from any starting point will lead to the global maximum.  \n\nBecause we will use the `minimize` function from `scipy.optimize` to find the maximum log-likelihood we'll minimize the negative log-likelihood:","metadata":{"hidden":true},"pos":42,"type":"cell"}
{"cell_type":"markdown","id":"5fd9ae","input":"This example is based on textbook problem 13.10-6 which is reproduced here:\n\nBecause of population growth, the state of Washington has been given an additional seat in the House of Representatives, making a total of 10. The state legislature, which is currently controlled by the Republicans, needs to develop a plan for redistricting the state. There are 18 major cities in the state of Washington that need to be assigned to one of the 10 congressional districts. The table below gives the numbers of registered Democrats and registered Republicans in each city. Each district must contain between 150,000 and 350,000 of these registered voters. Assign each city to one of the 10 congressional districts in order to maximize the number of districts that have more registered Republicans than registered Democrats.\n\n<img src=\"images/gerrymandering.png\" width=\"300\">\n\nWe'll provide the data and objective function below.","metadata":{"hidden":true},"pos":92,"type":"cell"}
{"cell_type":"markdown","id":"613b2e","input":"Most machine learning algorithms are driven by optimization.  Usually we want to minimize a loss function which measures the difference between the model predictions and the observed data.  Neural network training uses a version of the gradient descent algorithm to optimize the weights in the network.  Here we'll show how to fit a logistic regression model by maximizing a function.\n\nIn simple logistic regression we try to predict the value of the label $y$, which can be 0 or 1, for each value of a continuous predictor variable $x$.  In particular, the conditional probability that $y=1$ given the current value of $x$ is modeled by a sigmoid function (\"s\" curve) $$p(x) = \\frac{1}{1 + e^{-(b_0 + b_1 x)}}.$$\n\nFor example, the more hours a student studies to prepare for an exam, the higher the probability that they will pass the test.  Shown below is some data.  For each student we have the number of hours they studied and whether or not they passed the exam (1 for passed, 0 for failed).  This example data comes from the <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Wikipedia article on Logistic Regresssion</a>.","metadata":{"hidden":true},"pos":35,"type":"cell"}
{"cell_type":"markdown","id":"643841","input":"We can use the model to make predictions:","metadata":{"hidden":true},"pos":40,"type":"cell"}
{"cell_type":"markdown","id":"677753","input":"### <font color = \"blue\"> Self Assessment: Gerrymandering Local Search</font>","metadata":{"hidden":true},"pos":100,"type":"cell"}
{"cell_type":"markdown","id":"6e678d","input":"We'll have a look at an algorithm called \"2-opt\" that was proposed by Croes in 1958.  We won't focus on it too much since the idea doesn't really extend to other problems.  The main idea  is to reverse segments that cross over themselves to remove the cross over.  We loop repeatedly over all the possible reversals until there are no more cross overs.  Here is some Python to do 2-opt:","metadata":{"hidden":true},"pos":85,"type":"cell"}
{"cell_type":"markdown","id":"70a208","input":"### Video Walkthrough of this example","metadata":{"hidden":true},"pos":32,"type":"cell"}
{"cell_type":"markdown","id":"70b78c","input":"### <font color = \"blue\">Self Assessment:  Finding Multiple Extrema</font>","metadata":{"hidden":true},"pos":29,"type":"cell"}
{"cell_type":"markdown","id":"72b00b","input":"An apartment complex has 250 apartments to rent and that their profit in thousands of dollars is given by the function \n$$P(x) = -0.008 x^2 + 3.1 x - 80.$$\nFind the maximum profit and how many apartments to rent to achieve the maximum profit.  Use minimize from scipy.optimize to find the maximum.  Review the video above to see how to \"flip\" the problem to find a maximum.","metadata":{"hidden":true},"pos":25,"type":"cell"}
{"cell_type":"markdown","id":"7beabb","input":"Optimization with discrete variables tends to be more complicated than with continuous variables.  In the continuous case we can take advantage of calculus or numerical methods to compute gradient search directions that allow us to move to nearby points that are closer to optimal.  However with discrete random variables there is no generic way to compute better nearby points.  Often the best we can do is find nearby points, which is usually problem specific, and try them to see if they produce closer to optimal results.\n\nWe'll look carefully at the traveling salesman problem (TSP).  In addition to the information about the TSP in the textbook, there is copious information available on the internet.","metadata":{"hidden":true},"pos":67,"type":"cell"}
{"cell_type":"markdown","id":"7c3044","input":"# Optimization Basics (video)","metadata":{"heading_collapsed":true},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"831265","input":"## Wyndor Example","metadata":{"heading_collapsed":true,"hidden":true},"pos":8,"type":"cell"}
{"cell_type":"markdown","id":"88df54","input":"Start with a random state and move to a nearby state by changing one of the city assignments randomly.  You don't have to check for feasibility of each new state, just accept the move if you get a larger value of the fitness function.  Repeat until no progress is made for 1000 moves.  Your local search should be inside of a function with inputs the cities and the initial assignment.  The output should be the optimized assignment and the value of the fitness function.  \n\nNow write a loop that does 100 local searches.  Make sure the final, best solution is feasible.  What is the maximum number of districts that Republicans win?","metadata":{"hidden":true},"pos":101,"type":"cell"}
{"cell_type":"markdown","id":"88fc2d","input":"We'll be minimizing the total length of a tour that visits all 48 state capitols in the continental United States and ends back in the same city in which it begins.  The latitudes and longitudes of the cities are projected onto a rectangular coordinate system with $x$ and $y$ coordinates representing positions in meters which we convert to kilometers.  We use the `Matplotlib Basemap` package to do the projections and to draw routes on a map of the United States.  If you're trying to install Basemap at home make sure you're doing in a virtual environment that is not your base environment (as mentioned before, create a ds775 environment with conda).  Basemap doesn't install correctly, at least on macs, in the base environment.\n\nFirst we load the data and do the projections.  The $x$ and $y$ coordinates of the cities are stored in the 48 x 2 array `xy` with one pair per row.  We also define a function to visualize the tours.","metadata":{"hidden":true},"pos":76,"type":"cell"}
{"cell_type":"markdown","id":"8ac72d","input":"Let's see what this looks like for the TSP.  We'll use the subtour reversal algorithm, described in the textbook, to generate moves.  Here's what the local search looks like for the TSP:\n\n```\n choose a random tour  \n while shorter tours have been found in last max_tries\n     propose new tour with one random segment reversed\n     if acceptable (it will always be a valid tour) \n         compute new tour distance \n         if new shortest tour \n             remember it \n         else\n             reject new tour\n endwhile \n ```","metadata":{"hidden":true},"pos":72,"type":"cell"}
{"cell_type":"markdown","id":"8cb873","input":"Below is a graph of the data along with the graph of the fitted sigmoid function that models the probality of $y=1$ at each $x$.  Don't worry, we'll see where the fitted curve comes from in a bit.","metadata":{"hidden":true},"pos":38,"type":"cell"}
{"cell_type":"markdown","id":"8f2c90","input":"How many iterations does it take to reliably find the global minimum with dim = 3?  With dim = 4?  Use the multi-start strategy.","metadata":{"hidden":true},"pos":57,"type":"cell"}
{"cell_type":"markdown","id":"92e59e","input":"Approximately now many local searches are required to find the global minimum one time when dim = 10?  Is it surprising that you (very likely) didn't find it with 1000 local searches?  Explain","metadata":{"hidden":true},"pos":65,"type":"cell"}
{"cell_type":"markdown","id":"97bcb7","input":"The overall \"U\" shape is not surprising since for polynomials the behavior for large values of $x$ is determined by the highest degree term which is, in this case, $x^4$.  It appears that there is a minimum or minima close to the origin.  Let's zoom in a bit to see what we can:","metadata":{"hidden":true},"pos":19,"type":"cell"}
{"cell_type":"markdown","id":"99156a","input":"Note:  the approach outlined here for logisitic regression is very similar to the actual algorithms used by most software for computing logistic regression models. Many machine learning predictive models are trained by optimization.  To verify our results we check our results against those from Sci-kit Learn.  By default sklearn uses an L2 regularization term to avoid overfitting (more about this in DS740).  The amount of regularization is proportional to $1/C$ so we just use a huge $C$ to mimic no regularization.","metadata":{"hidden":true},"pos":48,"type":"cell"}
{"cell_type":"markdown","id":"9928a1","input":"## Another local search algorithm for TSP","metadata":{"heading_collapsed":true,"hidden":true},"pos":84,"type":"cell"}
{"cell_type":"markdown","id":"99ae54","input":"Notes:\n\n* Often the starting state is one selected at random.  State refers to the \"state\" or values of the variables.\n\n* The local condition is a stopping condition.  It could be something like stopping after a fixed number of iterations or stopping after making no or insignificant process for a while.\n\n* Selecting a move is where things get problem specific.  Often the move involves a random change to the variables.\n\n* If acceptable means that we are checking to see that the state is feasible, that is, does it satisfy the constraints?","metadata":{"hidden":true},"pos":70,"type":"cell"}
{"cell_type":"markdown","id":"9a4e56","input":"The function $f(x) = x^5 - x^4 - 18 x^3 + 16 x^2 + 32 x - 2$ for $-4 \\leq x \\leq 3.6$ appears in the video \"Gradient Descent and Local Minima\" above.  Plot the function on the given interval.  Use the graph to guess where the local maxima and minima are.  Now use minimize from scipy.optimize to find the $x$ and $y$ coordinates of all the extrema.  If you supply a bounds argument to `minimize` you may get unexpected results, instead use the graph to make reasonable guesses to the location of the optima.","metadata":{"hidden":true},"pos":30,"type":"cell"}
{"cell_type":"markdown","id":"9d5ba5","input":"<font size=18>Lesson 04: Quadratic Programming and Local Optimization</font>","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"a76c48","input":"## Example:  The Rastrigin Function","metadata":{"hidden":true},"pos":50,"type":"cell"}
{"cell_type":"markdown","id":"a8acc6","input":"## Local Search for TSP","metadata":{"heading_collapsed":true,"hidden":true},"pos":71,"type":"cell"}
{"cell_type":"markdown","id":"aa8188","input":"### <font color=\"blue\">Self-Assessment: Rastrigin with dim = 3, 4</font>","metadata":{"hidden":true},"pos":56,"type":"cell"}
{"cell_type":"markdown","id":"ae8273","input":"## The Basics of Local Search","metadata":{"heading_collapsed":true,"hidden":true},"pos":68,"type":"cell"}
{"cell_type":"markdown","id":"b10e78","input":"# Local Search - Continuous Variables","metadata":{"heading_collapsed":true},"pos":11,"type":"cell"}
{"cell_type":"markdown","id":"b132d9","input":"The `summarize_districts` function below isn't used directly in the initialization, but it helps us by printing out the number of voters (in thousands) assigned to each district and shows us which districts are won by republicans.","metadata":{"hidden":true},"pos":96,"type":"cell"}
{"cell_type":"markdown","id":"ba137f","input":"###  <font color = \"blue\">Self Assessment:  Minimize to Maximize</font>","metadata":{"code_folding":[],"hidden":true},"pos":24,"type":"cell"}
{"cell_type":"markdown","id":"bdec27","input":"For the objective function, `fitness_districts` we count the number of districts won by republicans.  Instead of checking each potential solution to see if it's feasible (between 150 and 350 voters in each district) we subtract the total number of thousands by which each district is out of bounds.  This approach to optimization is called a **penalty function method**.  It doesn't prevent infeasible solutions, but it strongly penalizes them so that when the function is optimized the solution generally will be feasible.  Note that in the table above we have several districts that too many or two few voters, when we compute the fitness of that assignment to districts it is negative because of the penalty term.","metadata":{"hidden":true},"pos":98,"type":"cell"}
{"cell_type":"markdown","id":"be317e","input":"There appears to be only one minimum somewhere around $x=0$ or $x=-1$.  Since the function appears to be convex, the starting point doesn't matter.  Let's search for the minimum beginning at $x_0 = -2$:","metadata":{"hidden":true},"pos":21,"type":"cell"}
{"cell_type":"markdown","id":"c30326","input":"## Example: Simple Logistic Regression","metadata":{"code_folding":[],"heading_collapsed":true,"hidden":true},"pos":31,"type":"cell"}
{"cell_type":"markdown","id":"c41adc","input":"While there are many options here, the defaults will serve well for our purposes.  The `method` specifies a variety of different numerical algorithms for local search.  We'll use `BFGS` for unbounded problems and `L-BFGS-B` for problems with bounds.  You shouldn't have to specify that choice as those are the defaults.  The BFGS methods are robust algorithms and are known as quasi-Newton methods.  What this means is that they approximate the shape of the objective function near the current point by approximating the derivatives (slopes and curvature) of the function and that shape information is used to produce an improved search point. \n\nOne of the things to pay attention to here is how we have to write our objective functions so that they can be passed to the `minimize` function.","metadata":{"hidden":true},"pos":15,"type":"cell"}
{"cell_type":"markdown","id":"c5f3e0","input":"In the cell below we load the data and create an assignment of the 18 cities to the 10 districts.","metadata":{"hidden":true},"pos":94,"type":"cell"}
{"cell_type":"markdown","id":"c5fb34","input":"## The curse of dimensionality","metadata":{"heading_collapsed":true,"hidden":true},"pos":60,"type":"cell"}
{"cell_type":"markdown","id":"c8988a","input":"Here we define both the local \"move\" function which reverses a tour segment to generate a new tour and the objective function which computes the length of the tour in kilometers:","metadata":{"hidden":true},"pos":78,"type":"cell"}
{"cell_type":"markdown","id":"ca7ffe","input":"### Setup for Logistic Regression","metadata":{"hidden":true},"pos":34,"type":"cell"}
{"cell_type":"markdown","id":"d4d8b5","input":"# Quadratic Programming","metadata":{"heading_collapsed":true},"pos":6,"type":"cell"}
{"cell_type":"markdown","id":"d512fd","input":"### A univariate function","metadata":{"hidden":true},"pos":16,"type":"cell"}
{"cell_type":"markdown","id":"d69f82","input":"## Gerrymandering Example","metadata":{"heading_collapsed":true,"hidden":true},"pos":91,"type":"cell"}
{"cell_type":"markdown","id":"e080bb","input":"Your answer shouldn't be an integer.  We could use discrete optimization and only optimize integer numbers of apartments, but it will usually be more computationally intensive.  Instead we're using a continuous variable to get an approximation to the discrete problem, this is called **relaxation** (we've relaxed the integer variable condition).  So what whole number of apartments should you rent?  Why? ","metadata":{"hidden":true},"pos":28,"type":"cell"}
{"cell_type":"markdown","id":"e56c49","input":"Local search algorithms for continuous variables are generally based on approximating the objective function near the current search point, then using that approximation to compute an improved search point.  For instance if we can calculate the gradient (calculus) or approximate it, then a move along the gradient direction will increase the value of the function.  \n\nWe'll primarily use the `scipy.optimize` function `minimize` for local search on continuous functions.  You can read more about it below.  ","metadata":{"hidden":true},"pos":12,"type":"cell"}
{"cell_type":"markdown","id":"e8815e","input":"The Rastrigin function is a common test case for optimization algorithms because it has many local minima.  The definition of the function is \n$$f(\\mathbf{x})=10 n+\\sum_{i=1}^{n}\\left[x_{i}^{2}-A \\cos \\left(2 \\pi x_{i}\\right)\\right]$$\nWhere $n$ is the dimensionality of input vector $\\mathbf{x}$.  For instance if $n=2$ then $\\mathbf{x} = (x_1, x_2)$.  The domain is restricted so that each $x_i \\in [-5.12, 5.12].$ .   Here is a graph of the the Rastrigin function with dimension $n=1.$","metadata":{"hidden":true},"pos":51,"type":"cell"}
{"cell_type":"markdown","id":"eb3c83","input":"The Rastrigin function isn't important as a real-life example, but it does serve as a good test problem with oodles of local minima and we know that global minimum occurs at the origin.  This is similar to what can happen in training in neural networks and other complex models except that we don't know where the global optimum is.\n\nA simple approach for trying to find the global minimum of a multi-modal function is called a **restart** or **multistart strategy** in which local searches are started at randomly generated initial points and the most optimal result of all the local searches is recorded.\n\nHere is pseudo-code for a multistart code:\n```\nfor num_searches:\n choose random initial state\n do local search\n if new optimum\n     remember it\nendfor\n```\n","metadata":{"hidden":true},"pos":54,"type":"cell"}
{"cell_type":"markdown","id":"f3eb57","input":"### <font color = \"blue\">Self-Assessment:  Rastrigin with dim = 10 </font>","metadata":{"hidden":true},"pos":58,"type":"cell"}
{"cell_type":"markdown","id":"f72cb3","input":"## Minimize from scipy.optimize","metadata":{"heading_collapsed":true,"hidden":true},"pos":13,"type":"cell"}
{"cell_type":"markdown","id":"f74d49","input":"## Video for TSP Local Search Code","metadata":{"heading_collapsed":true,"hidden":true},"pos":73,"type":"cell"}
{"cell_type":"markdown","id":"fbd62d","input":"You should read about quadratic programming in the textbook.  In short, the constraints are the same as they are in linear programming and the objective function can have degree 2 and interaction terms.  In Pyomo it is only a matter of changing the solver to one capable of solving quadratic programs, `ipopt`, instead of `glpk`.  Other solvers like CPLEX, a commercial solver, could also be used.  You  may need to install `ipopt` using conda on your own machine.","metadata":{"hidden":true},"pos":7,"type":"cell"}
{"last_load":1569540029236,"type":"file"}